# utils/agent_orchestrator.py

from utils.planner_agent import plan_tools_for_query
from utils.searcher_agent import search_web
from utils.navigator_agent import navigate_and_capture
from utils.response_generator import generate_final_answer
from utils.validator_agent import validate_schema_against_usecase, extract_metadata_type
from utils.cache_manager import GlobalCache, is_public_query

# Optional import for scraper (may fail in some environments)
try:
    from utils.web_retriever import run_scraper
except ImportError:
    run_scraper = None


def classify_intent_and_usecase(query: str):
    """
    Temporary rule-based intent and use-case classifier.
    Replace with Cohere or Gemini if needed.
    """
    query = query.lower()
    if "repo" in query or "interest rate" in query or "rbi" in query:
        return "GetRates", "Banking Norms"
    if "headquarter" in query or "where is hdfc" in query:
        return "FindLocation", "Documentation & Process Query"
    if "credit card" in query:
        return "BrowseProduct", "Documentation & Process Query"
    if "loan" in query and "type" in query:
        return "BrowseProduct", "Loan Prepurchase Query"
    if "emi calculator" in query:
        return "ToolUse", "Loan Prepurchase Query"
    return "Generic", "Internal Account"


def orchestrate_agents(query, use_case=None, user_name="Customer"):
    """
    Full agent pipeline:
    Planner ‚Üí Searcher ‚Üí Navigator ‚Üí Scraper ‚Üí Validator ‚Üí Cache + Response
    """
    print("\n[Agent Pipeline] Starting agent chain for:", query)

    # 1Ô∏è‚É£ Classify intent and use-case
    intent, predicted_use_case = classify_intent_and_usecase(query)
    use_case = use_case or predicted_use_case
    print(f"[Planner] Intent: {intent} | Use Case: {use_case}")

    # 2Ô∏è‚É£ Tool planning
    if is_public_query(intent, use_case):
        tools = plan_tools_for_query(query)
    else:
        tools = ["none"]
    print(f"[Planner] Toolchain decided: {tools}")

    search_results = None
    html_content = None
    scraped = None
    context = None
    top_link = None

    try:
        for tool in tools:
            if tool == "search":
                search_results = search_web(query)
                if not search_results or not search_results[0][1]:
                    return "‚ö†Ô∏è No relevant search results found."
                top_link = search_results[0][1]
                print(f"[Searcher] Top link: {top_link}")

            elif tool == "navigate":
                if not top_link:
                    return "‚ö†Ô∏è Cannot navigate: No link available."
                html_content = navigate_and_capture(top_link)
                if not html_content:
                    return "‚ö†Ô∏è Failed to capture page content."
                print("[Navigator] HTML captured.")

            elif tool == "scrape":
                if not html_content:
                    return "‚ö†Ô∏è No HTML to scrape."
                if not run_scraper:
                    return "‚ùå Scraper function not available in web_retriever."
                scraped = run_scraper(html_content)
                if not scraped:
                    return "‚ö†Ô∏è Scraping failed or returned empty data."
                print("[Scraper] Data extracted.")

            elif tool == "validate":
                if not scraped:
                    return "‚ö†Ô∏è Nothing to validate."
                if not validate_schema_against_usecase(use_case, scraped):
                    return "‚ö†Ô∏è Retrieved data didn't match expected format."
                print("[Validator] Schema validated ‚úÖ")

            elif tool == "link_resolver":
                return "üîó Tool not implemented yet. Try using Gemini link resolver directly."

            elif tool == "none":
                print("‚ÑπÔ∏è Planner chose no tools ‚Äî skipping agent pipeline.")
                return "NO_OP"

        # 3Ô∏è‚É£ Generate response from scraped data
        if scraped:
            final_response = generate_final_answer(query, scraped, user_name=user_name)
            source = extract_metadata_type(scraped)

            # 4Ô∏è‚É£ Cache it
            GlobalCache.set(
                query=query,
                response=final_response,
                source=source,
                use_case=use_case,
                validated=True
            )
            return final_response

        return "‚ö†Ô∏è Agent pipeline completed but no response could be generated."

    except Exception as e:
        return f"‚ùå Agent pipeline crashed: {e}"
